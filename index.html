<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta http-equiv=“Pragma” content=”no-cache”>
  <meta http-equiv=“Expires” content=”-1″>
  <meta http-equiv=“CACHE-CONTROL” content=”NO-CACHE”>

  <!-- Bootstrap CSS -->
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <!-- Negative Momentum -->
  <link rel="stylesheet" type="text/css" href="css/eigenvalues.css">
  <link rel="stylesheet" type="text/css" href="css/colors.css">
  <link rel="stylesheet" type="text/css" href="main.css">
  <link rel="icon" type="image/png" href="img/favico.png">

  <title>Gabriel Huang</title>
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-8LH518FP4T"></script>
  <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8LH518FP4T');
  </script>
  
</head>

<body>

  <!-- Optional JavaScript -->
  <!-- jQuery first, then Popper.js, then Bootstrap JS -->
  <script src="js/jquery-3.5.1.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"
    integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1"
    crossorigin="anonymous"></script>
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"
    integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM"
    crossorigin="anonymous"></script>





  <div class="container">

    <nav>
      <a class="" href="#publications">Publications</a> |
      <a class="" href="https://scholar.google.com/citations?user=_X1Tl-MAAAAJ">Google Scholar</a> |
      <a class="" href="https://github.com/gabrielhuang/">Github</a> |
      <a class="" href="https://naivepsychology.com">Naive Psychology</a> |
      <a class="" href="https://www.linkedin.com/in/gbxhuang/">LinkedIn</a> |
      <a class="" href="./gabriel-huang-march-2022.pdf">CV</a>

    </nav>

    <div style="width: 100%; display: flex; text-align: center; align-items: center; margin: 2ex 0;  justify-content: center;">
        <div style="flex: 30%;">
          <br>
          <p class="lead">Gabriel Huang (he/him)</p>
          <p>PhD, Mila &amp; University of Montreal<br>
            Research Scientist, ServiceNow</p>
        </div>
      <div style="flex: 30%"><img style="border-radius: 50%;" src="id-flower.jpg" width="160" height="160" alt="gabriel huang profile picture"></div>
    </div>


    <br><br>

I am a research scientist at ServiceNow working on the 
reliability and security of autonomous AI agents with access to APIs and UIs. 
My collaborators and I have released [DoomArena](https://servicenow.github.io/DoomArena/), a framework for agentic security. 
Previously, we worked on synthetic dialogue generation for AI agent distillation and evaluation, simulated users, 
company APIs and policies, with an emphasis on rigourous evaluation of our systems using both human labelers and LLM-as-a-Judge, 
resulting in the release of the [TapeAgents](https://github.com/ServiceNow/tapeagents) agentic framework.

I did my PhD at <a href="https://mila.quebec/en/">Mila Quebec AI Institute, Université de Montréal </a> with 
<a href="http://www.iro.umontreal.ca/~slacoste/">Simon Lacoste-Julien</a>.
I interned at <b>Google Research</b> working on multimodal pretraining for video captioning.
I also hold two MSc. from <b>Ecole Normale Supérieure</b> and <b>CentraleSupélec</b>, where I specialized in machine learning, computer vision and statistics.
   
<!-- 
    <p><b>Update March 2022</b>: I am looking for a full-time position to pursue research on <b>Responsible&nbsp;AI</b>.<br>
      <i>Topics of interest: impact of AI on human behavior and social dynamics,
      ethical and emotional AI,
      aligning AI with humanist values,
      self-supervised and low-data learning, 
      multimodal transformers, 
      and large language models.
    </i>
    </p>

    <p><u>Aligning AI with technical goals</u> was a central theme of my PhD thesis,
      during which I have developed technical skills on multimodal language models, self-supervised and few-shot learning, object detection, optimal transport and generative learning.
      Now, I want to apply these technical skills to develop responsible and ethical AI <u>aligned with humanist values</u>, 
      not only to ensure that individuals are treated fairly,
      but also to strengthen the core values of our societies (e.g. human life, truth, democracy, sustainability).
      I have recently started <a href="https://naivepsychology.com/">Naive Psychology</a>, a blog to share my ideas on the interplay between AI, human behavior and society.
    </p>

    <p>
      <b>About me</b>: I am a PhD student at <a href="https://mila.quebec/en/">Mila Quebec AI Institute</a> advised by <a
        href="http://www.iro.umontreal.ca/~slacoste/">Simon Lacoste-Julien</a>.
      Currently, I am a visiting researcher at <b>ServiceNow</b> (ElementAI)
      working on few-shot and self-supervised object detection, low-data language models,
      and self-supervised representations for climate change monitoring.
      Previously, I was an intern at <b>Google Research</b> working on multimodal pretraining for video captioning.
      I also hold two MSc. from <b>Ecole Normale Supérieure</b> and <b>CentraleSupélec</b>, where I specialized in machine learning, computer vision and statistics.
    </p> -->


      <!-- 
          Previously he did the <a href="http://www.math.ens-cachan.fr/version-francaise/formations/master-mva/contenus-/master-mva-cours-2016-2017-161721.kjsp?RH=1242430202531">MVA Master's degree</a>
          in machine learning at  École Normale Supérieure in Paris,
          in parallel with an engineer's degree at CentraleSupélec,
          one of the top engineering schools in France.
          While he was doing his master's, I also worked as a part-time research apprentice on
          human activity recognition using RGB-D cameras and on recommender systems.
          -->
    </p>
    <h4> <a name="publications">Publications</a> </h4>


    <div class="papers">
      <div>
        <div class="img">
          <img src="img/fsod.jpg">
        </div>
        <div class="text">
          <span class="badge badge-secondary">arxiv</span>
          <span class="pt"><a href="https://gabrielhuang.github.io/fsod-survey/">A Survey of Self-Supervised and
              Few-Shot Object Detection</a></span> <br>
          Gabriel Huang, Issam Laradji, David Vazquez, Simon Lacoste-Julien, Pau Rodriguez<br>
          Submitted to IEEE TPAMI.<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/2110.14711" role="button">arXiv</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://gabrielhuang.github.io/fsod-survey/"
              role="button">project page</a>
            <a class="btn btn-outline-secondary btn-sm"
              href="https://twitter.com/GabrielHuang9/status/1455639917497827334" role="button">tweetorial</a>
            <a class="btn btn-outline-secondary btn-sm"
              href="https://github.com/gabrielhuang/awesome-few-shot-object-detection" role="button">Awesome-FSOD</a>
          </div>
        </div>
      </div>
      <div>
        <div class="img">
          <img src="img/repurposing.png">
        </div>
        <div class="text">
          <span class="badge badge-success">paper</span>
          <span class="pt"><a href="https://arxiv.org/pdf/2103.09027.pdf">Repurposing Pretrained Models for Robust
              Out-of-domain Few-Shot Learning</a>
          </span> <br>
          Namyeong Kwon, Hwidong Na, Gabriel Huang, Simon Lacoste-Julien<br>
          ICLR'21 paper.<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/2103.09027" role="button">arXiv</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/pdf/2103.09027.pdf"
              role="button">pdf</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://openreview.net/forum?id=qkLMTphG5-h"
              role="button">openreview</a>
          </div>
        </div>
      </div>
      <div>
        <a name="multimodal-pretraining-paper"></a>
        <div class="img">
          <img src="img/multimodal-pretraining.png">
        </div>
        <div class="text">
          <span class="badge badge-success">paper</span>
          <span class="pt"><a href="https://arxiv.org/pdf/2011.11760.pdf">Multimodal Pretraining for Dense Video
              Captioning</a></span> <br>
          Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, Radu Soricut<br>
          <i>Introduces the Video Timeline Tags dataset (<a
              href="https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT">ViTT</a>).</i> <br>
          AACL-IJCNLP 2020.<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/2011.11760" role="button">arXiv</a>
            <a class="btn btn-outline-secondary btn-sm"
              href="https://github.com/google-research-datasets/Video-Timeline-Tags-ViTT" role="button">dataset</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://youtu.be/hKSDJSDRee8" role="button">video</a>
            <a class="btn btn-outline-secondary btn-sm"
              href="https://docs.google.com/presentation/d/13xu7UxdRHawI2lmgb8YF74SJ-g5VWtTF7mo5LqBwOv0/edit?usp=sharing"
              role="button">slides</a>
          </div>
        </div>
      </div>
      <div>
        <div class="img">
          <img src="img/centroid.jpg">
        </div>
        <div class="text">
          <span class="badge badge-secondary">arXiv</span>
          <span class="pt"><a href="https://arxiv.org/pdf/1902.08605.pdf">Are Few-Shot Learning Benchmarks too Simple ?
              Solving them without Task Supervision at Test-Time</a>
          </span> <br>
          This paper introduces <i>Centroid Networks</i> for Few-shot Clustering and Unsupervised Few-shot
          Classification<br>
          Gabriel Huang, Hugo Larochelle, Simon Lacoste-Julien<br>
          ICLR'19 workshop.<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/1902.08605" role="button">arXiv</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/pdf/1902.08605.pdf"
              role="button">pdf</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://github.com/gabrielhuang/centroid-networks"
              role="button">code</a>
          </div>
        </div>
      </div>
      <div>
        <div class="img">
          <img src="img/negative-momentum.jpg">
        </div>
        <div class="text">
          <span class="badge badge-success">paper</span>
          <span class="pt"><a name="negative_momentum_paper"><a href="https://arxiv.org/pdf/1807.04740.pdf">Negative
                Momentum for Improved Game Dynamics</a></a></span> <br>
          Gauthier Gidel, Reyhane Askari Hemmat, Mohammad Pezeshki, Gabriel Huang, Rémi Lepriol, Simon Lacoste-Julien,
          Ioannis Mitliagkas.<br>
          AISTATS 2019<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/1807.04740" role="button">arXiv</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://gauthiergidel.github.io/slides/TTIC2018.pdf"
              role="button">slides</a>
            <a class="btn btn-outline-secondary btn-sm" href="#negative_momentum_visualization"
              role="button">visualization</a>
          </div>
        </div>
      </div>
      <div>
        <div class="img">
          <img src="img/parametric.jpg">
        </div>
        <div class="text">
          <span class="badge badge-secondary">arXiv</span>
          <span class="pt"><a href="https://arxiv.org/pdf/1708.02511.pdf">Parametric Adversarial Divergences are Good
              Task Losses for Generative Modeling</a></span> <br>
          Gabriel Huang, Hugo Berard, Ahmed Touati, Gauthier Gidel, Pascal Vincent, Simon Lacoste-Julien. <br>
          ICML'17 Workshop, ICLR'18 Workshop, Montreal AI Symposium 2018, Submitted to JMLR<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/abs/1708.02511" role="button">arxiv</a>
            <a class="btn btn-outline-secondary btn-sm" href="https://arxiv.org/pdf/1708.02511.pdf"
              role="button">pdf</a>
            <a class="btn btn-outline-secondary btn-sm"
              href="https://openreview.net/forum?id=rkEtzzWAb&noteId=BkiY4k6rM" role="button">openreview</a>
          </div>
        </div>
      </div>
      <div>
        <div class="img">
          <img src="img/scattering.jpg">
        </div>
        <div class="text">
          <span class="badge badge-success">paper</span>
          <span class="pt"><a href="https://hal.inria.fr/hal-01837587/file/main.pdf">Scattering Networks for Hybrid
              Representation Learning</a></span> <br>
          Edouard Oyallon, Sergey Zagoruyko, Gabriel Huang, Nikos Komodakis,
          Simon Lacoste-Julien, Matthew Blaschko, Eugene Belilovsky.<br>
          IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 2018<br>
          <div class="btn-group">
            <a class="btn btn-outline-secondary btn-sm" href="https://hal.inria.fr/hal-01837587/file/main.pdf">paper</a>
          </div>
        </div>
      </div>
    </div>


    <!-- Negative momentum -->
    <script src="https://d3js.org/d3.v5.min.js"></script>
    <script src="js/math.min.js"></script>
    <script src="js/eigenvalues.js"></script>
    <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

    <h4> <a name="negative_momentum_visualization">Negative Momentum</a> </h4>

    <p>Below is an interactive visualization of our paper <a href="#negative_momentum_paper">Negative Momentum for
        Improved Game Dynamics</a>:<br>
      (a) Learning rate (<b>lr</b>) and momentum (<b>beta</b>) hyperparameters.<br>
      (b) Resulting eigenvalues in the complex plane for <u class="sgd">SGD</u> and <u
        class="sgdmomentum">SGD+momentum</u>. </p>
    <p>There is convergence if and only if all eigenvalues are inside the convergence ball (green). <br>Try to find the
      hyperparameters for convergence.</p>

    <div style="text-align:center">
      <div style="text-align:center; display:inline-block">
        <svg class="touchpad">
          <p>(a) Hyperparameters.
        </svg>
      </div>
      <div style="text-align:center; display:inline-block;">
        <svg class="chart">
          <p>(b) Eigenvalues in complex plane.
        </svg>
      </div>


    </div>

    <h5></h5>


    <p><u class="sgd">SGD without momentum:</u> using <span class="snm-span p_lr b"></span>,
      eigenvalues are <span class="p_sgd_inside b"></span> the convergence ball &rarr; <span
        class="b p_sgd_converges"></span><br>

      <u class="sgdmomentum">SGD with momentum:</u> using <span class="snm-span p_lr b"></span>
      and <span class="p_sign_beta b"></span> momentum <span class="p_beta b"></span>,
      eigenvalues are <span class="p_momentum_inside b"></span> the convergence ball &rarr; <span
        class="b p_momentum_converges"></span>
    </p>



    <script>
      $(document).ready(function () {
        on_resize();
      })

      $(window).resize(function () {
        on_resize();
      });

      function on_resize() {
        if ($('.papers').width() < 600) {
          // image on top
          $('.papers .img').css('float', 'none');
          $('.papers .text').css('width', '100%');
        } else {
          // side by side
          $('.papers .img').css('float', 'left');
          $('.papers .text').css('width', ($('.papers').width() - 250) + 'px');
        }
      }
    </script>

    <h4><a name="thin">Thin-8 dataset</a></h4>

    <p>The Thin-8 dataset consists of <b>1585</b> grayscale handwritten images of the digit 8, with resolution
      512x512.<br>
      16 people were asked to draw the digit 8 about 100 times using a pen on a tablet PC running Microsoft Windows.<br>
      It was collected in October 2017 at the University of Montreal.<br>
      <a href="https://drive.google.com/file/d/1TTgBm2eepo4vGU8UlGa6SbhvXoADArPP/view">Download Thin-8 dataset here</a>
    </p>

    <img src="img/thin-8.jpg" width="80%" style="margin: 1em;">

    <p>If you use the Thin-8 dataset, please cite <a href="https://arxiv.org/abs/1708.02511">our paper </a>:</p>

    <pre><code>@article{huang2018parametric,
                    title={Parametric Adversarial Divergences are Good Task Losses for Generative Modeling},
                    author={Huang, Gabriel and Berard, Hugo and Touati, Ahmed and Gidel, Gauthier and Vincent, Pascal and Lacoste-Julien, Simon},
                    journal={arXiv preprint arXiv:1708.02511},
                    year={2017}
                  }</code></pre>

    <p>Thanks to Alex, Akram, Aristide, David, Dendi, Eugene,
      Jae, Joao, Liam, Rémi, Rosemary, Shawn, Sina, and Xing for scribbling all those samples!</p>



    <h4><a name="thin">Twitter Feed</a></h4>
    <!-- Twitter Feed -->
    <a class="twitter-timeline" data-width="600" data-height="800"
      href="https://twitter.com/GabrielHuang9?ref_src=twsrc%5Etfw">Tweets by GabrielHuang9</a>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>


    <h4>Contact</h4>

    <p>Email: <a href="mailto:gbxhuang@gmail.com">gbxhuang@gmail.com</a><br>
      In person: Mila, 6666 St-Urbain, #200, Montreal, QC, H2S 3H1, Canada
    </p>

</body>

</html>
